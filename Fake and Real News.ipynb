{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Final Project.ipynb","provenance":[{"file_id":"https://github.com/Zimi1214/Final-Project/blob/master/Final_Project.ipynb","timestamp":1588163277149}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"aLkcXmfR1V93","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1588224872552,"user_tz":-120,"elapsed":33209,"user":{"displayName":"Yehia Khaled Abouzeid","photoUrl":"","userId":"09822096430229382785"}},"outputId":"a909028f-179c-4035-f02c-280291394767"},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","import string\n","from nltk.stem.porter import PorterStemmer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wYXlCuzOOoiU","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import string\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score, GridSearchCV\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.pipeline import Pipeline\n","import xgboost as xgb\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from keras import Sequential\n","from keras.layers import Embedding, LSTM, Dense, Dropout\n","from keras.models import load_model\n","\n","\n","seed = 4353"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ikt0nC826dyY","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWRcSaTpgS9M","colab_type":"text"},"source":["# Code to read csv file into Colaboratory:\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"]},{"cell_type":"code","metadata":{"id":"Hb2DzGQSgm2W","colab_type":"code","colab":{}},"source":["link_fake='https://drive.google.com/open?id=1sTzWwA8JCiBKH9u364c-kezhMnTiHsvI'\n","fluff, id1 = link_fake.split('=')\n","print (id1) # Verify that you have everything after '='"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6tRpwi8j6O0","colab_type":"code","colab":{}},"source":["link_true='https://drive.google.com/open?id=1fhYamoD4zvINmJF33sxhaMz7ynpmpsN9'\n","fluff, id2 = link_true.split('=')\n","print (id2) # Verify that you have everything after '='"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfnk1k35j5dY","colab_type":"code","colab":{}},"source":["downloaded = drive.CreateFile({'id':id1}) \n","downloaded.GetContentFile('Fake.csv')\n","downloaded = drive.CreateFile({'id':id2})\n","downloaded.GetContentFile('True.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kiqofHQhj68w","colab_type":"code","colab":{}},"source":["df_fake=pd.read_csv('Fake.csv')\n","df_true=pd.read_csv('True.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJaTcE-ldAhW","colab_type":"code","colab":{}},"source":["df_fake.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqNbvZwWdbkR","colab_type":"code","colab":{}},"source":["df_fake.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nj3BpwFYdgAj","colab_type":"code","colab":{}},"source":["df_fake.describe(include='all')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqkX2MeUdnV5","colab_type":"code","colab":{}},"source":["df_fake['subject'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adi623N1drtt","colab_type":"code","colab":{}},"source":["df_fake['text']=df_fake['title'].astype(str)+df_fake['text'].astype(str)\n","df_fake['type']=0\n","df_fake.drop(columns=['title','date'],inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaK-2ti_dtwB","colab_type":"code","colab":{}},"source":["df_true.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IlzNXQaEdw45","colab_type":"code","colab":{}},"source":["df_true['text']=df_true['title'].astype(str)+df_true['text'].astype(str)\n","df_true['type']=1\n","df_true.drop(columns=['title','date'],inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9Q7aCtkd1CW","colab_type":"code","colab":{}},"source":["df_true['subject'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_flls46Wd_ne","colab_type":"code","colab":{}},"source":["df=pd.concat([df_fake,df_true])\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SxmDjH-xeCu8","colab_type":"code","colab":{}},"source":["df.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQhDiijfeIhK","colab_type":"code","colab":{}},"source":["df.describe(include='all')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_7pPkGkeN62","colab_type":"code","colab":{}},"source":["df['type'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6ZN9-3VeSEl","colab_type":"code","colab":{}},"source":["df['subject'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6RV2MCKeW9X","colab_type":"code","colab":{}},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MU5F9bz4ecdC","colab_type":"code","colab":{}},"source":["df.iloc[0,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rss8yCg_ehMA","colab_type":"code","colab":{}},"source":["porter = PorterStemmer()\n","stop_words = stopwords.words('english')\n","\n","for i in np.arange(0,44898):\n","    text_tokens = word_tokenize(df.iloc[i,0])\n","    text_tokens = [w.lower() for w in text_tokens]\n","    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n","    table = str.maketrans('', '', string.punctuation)\n","    stripped = [w.translate(table) for w in tokens_without_sw]\n","    words = [word for word in stripped if word.isalpha()]\n","    stemmed = [porter.stem(word) for word in words]\n","    join_back = (\" \").join(stemmed)\n","    df.iloc[i,0] = join_back"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bd-rgXPpn2l","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6GhKla5CfDxB","colab_type":"code","colab":{}},"source":["df.iloc[0,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4WT8o45ntyD2","colab_type":"code","colab":{}},"source":["df.to_csv('clean.csv')\n","!cp clean.csv \"drive/My Drive/Data Science\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mnsya8P0KvZv","colab_type":"code","colab":{}},"source":["link_clean='https://drive.google.com/open?id=1pC7daDCEmdINGjTqDMEl60QZucLPc290'\n","fluff, id3 = link_clean.split('=')\n","print (id3) # Verify that you have everything after '='"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EK2Ne61iLQmx","colab_type":"code","colab":{}},"source":["downloaded = drive.CreateFile({'id':id3}) \n","downloaded.GetContentFile('clean.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNuGLpu4sW9M","colab_type":"code","colab":{}},"source":["df=pd.read_csv('/content/drive/My Drive/Data Science/clean.csv')\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hzvcvTcKvqC","colab_type":"code","colab":{}},"source":["# Word extraction from true and fake texts\n","\n","true_text = df[df.type==1]['text']\n","true_text = true_text.reset_index().drop(['index'], axis=1)\n","fake_words = df[df.type==0]['text']\n","fake_words = fake_words.reset_index().drop(['index'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cb33tYKiKkWA","colab_type":"code","colab":{}},"source":["def plot_wordcloud(text_d):\n","    wordcloud = WordCloud(background_color = 'black',\n","                         max_words = 3000,\n","                         width=1600,\n","                         height=800).generate(text_d)\n","    plt.clf()\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis('off')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEEBE93xK50J","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(20,18))\n","plot_wordcloud(\" \".join(true_text.text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"plDKX3WLK6O4","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(20,18))\n","plot_wordcloud(\" \".join(fake_words.text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kx2vZxk4Lv9c","colab_type":"code","colab":{}},"source":["# Function to retrieve processed words\n","\n","def final(X_data_full):\n","    \n","    # function for removing punctuations\n","    def remove_punct(X_data_func):\n","        string1 = X_data_func.lower()\n","        translation_table = dict.fromkeys(map(ord, string.punctuation),' ')\n","        string2 = string1.translate(translation_table)\n","        return string2\n","    \n","    X_data_full_clear_punct = []\n","    for i in range(len(X_data_full)):\n","        test_data = remove_punct(X_data_full[i])\n","        X_data_full_clear_punct.append(test_data)\n","        \n","    # function to remove stopwords\n","    def remove_stopwords(X_data_func):\n","        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n","        string2 = pattern.sub(' ', X_data_func)\n","        return string2\n","    \n","    X_data_full_clear_stopwords = []\n","    for i in range(len(X_data_full)):\n","        test_data = remove_stopwords(X_data_full[i])\n","        X_data_full_clear_stopwords.append(test_data)\n","        \n","    # function for tokenizing\n","    def tokenize_words(X_data_func):\n","        words = nltk.word_tokenize(X_data_func)\n","        return words\n","    \n","    X_data_full_tokenized_words = []\n","    for i in range(len(X_data_full)):\n","        test_data = tokenize_words(X_data_full[i])\n","        X_data_full_tokenized_words.append(test_data)\n","        \n","    # function for lemmatizing\n","    lemmatizer = WordNetLemmatizer()\n","    def lemmatize_words(X_data_func):\n","        words = lemmatizer.lemmatize(X_data_func)\n","        return words\n","    \n","    X_data_full_lemmatized_words = []\n","    for i in range(len(X_data_full)):\n","        test_data = lemmatize_words(X_data_full[i])\n","        X_data_full_lemmatized_words.append(test_data)\n","        \n","    # creating the bag of words model\n","    cv = CountVectorizer(max_features=1000)\n","    X_data_full_vector = cv.fit_transform(X_data_full_lemmatized_words).toarray()\n","    \n","    \n","    tfidf = TfidfTransformer()\n","    X_data_full_tfidf = tfidf.fit_transform(X_data_full_vector).toarray()\n","    \n","    return X_data_full_tfidf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C43WklfsL2tI","colab_type":"code","colab":{}},"source":["# Preparing training and testing data using train_test_split\n","\n","# Data preparation\n","\n","import nltk\n","nltk.download('wordnet')\n","\n","X_data = df['text']\n","y_data = df.type\n","X_data = X_data.astype(str)\n","\n","x_data=final(X_data)\n","\n","NBX_train, NBX_test, NBy_train, NBy_test = train_test_split(x_data, y_data, test_size=0.3, random_state= seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1MS5ideL78O","colab_type":"code","colab":{}},"source":["# Instatiation, fitting and prediction\n","\n","MNB = MultinomialNB()\n","MNB.fit(NBX_train, NBy_train)\n","NB_predictions = MNB.predict(NBX_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9X4sYf9qL_oW","colab_type":"code","colab":{}},"source":["# Model evaluation\n","\n","print(\"Classification Report\", \"\\n\" ,classification_report(NBy_test, NB_predictions))\n","print(\"Confusion Matrix\", \"\\n\" ,confusion_matrix(NBy_test, NB_predictions))\n","\n","MNB_f1 = round(f1_score(NBy_test, NB_predictions, average='weighted'), 3)\n","MNB_accuracy = round((accuracy_score(NBy_test, NB_predictions)*100),2)\n","\n","print(\"\\n\",\"NB Evaluation:\",\"\\n\",\"Accuracy : \" , MNB_accuracy , \" %\")\n","print(\"f1_score : \" , MNB_f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-x27Dbc6MA9I","colab_type":"code","colab":{}},"source":["# Instatiation, fitting and prediction\n","\n","RFX_train, RFX_test, RFy_train, RFy_test = train_test_split(x_data, y_data, test_size=0.3, random_state= seed)\n","\n","rfc=RandomForestClassifier(n_estimators= 10, random_state= seed)\n","rfc.fit(RFX_train, RFy_train)\n","RF_predictions = rfc.predict(RFX_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55tUA4KsMFtO","colab_type":"code","colab":{}},"source":["# Model evaluation\n","\n","print(\"Classification Report\", \"\\n\" , classification_report(RFy_test, RF_predictions))\n","print(\"Confusion Matrix\", \"\\n\" ,confusion_matrix(RFy_test, RF_predictions))\n","\n","rfc_f1 = round(f1_score(RFy_test, RF_predictions, average= 'weighted'), 3)\n","rfc_accuracy = round((accuracy_score(RFy_test, RF_predictions) * 100), 2)\n","\n","print(\"\\n\",\"RF Evaluation:\",\"\\n\",\"Accuracy : \" , rfc_accuracy , \" %\")\n","print(\"f1_score : \" , rfc_f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvf0tNIbUlIX","colab_type":"code","colab":{}},"source":["# Function to extract major words from true and fake news\n","\n","def wordcloud_words(X_data_full):\n","    \n","    # function for removing punctuations\n","    def remove_punct(X_data_func):\n","        string1 = X_data_func.lower()\n","        translation_table = dict.fromkeys(map(ord, string.punctuation),' ')\n","        string2 = string1.translate(translation_table)\n","        return string2\n","    \n","    X_data_full_clear_punct = []\n","    for i in range(len(X_data_full)):\n","        test_data = remove_punct(X_data_full[i])\n","        X_data_full_clear_punct.append(test_data)\n","        \n","    # function to remove stopwords\n","    def remove_stopwords(X_data_func):\n","        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n","        string2 = pattern.sub(' ', X_data_func)\n","        return string2\n","    \n","    X_data_full_clear_stopwords = []\n","    for i in range(len(X_data_full)):\n","        test_data = remove_stopwords(X_data_full[i])\n","        X_data_full_clear_stopwords.append(test_data)\n","        \n","    # function for tokenizing\n","    def tokenize_words(X_data_func):\n","        words = nltk.word_tokenize(X_data_func)\n","        return words\n","    \n","    X_data_full_tokenized_words = []\n","    for i in range(len(X_data_full)):\n","        test_data = tokenize_words(X_data_full[i])\n","        X_data_full_tokenized_words.append(test_data)\n","        \n","    # function for lemmatizing\n","    lemmatizer = WordNetLemmatizer()\n","    def lemmatize_words(X_data_func):\n","        words = lemmatizer.lemmatize(X_data_func)\n","        return words\n","    \n","    X_data_full_lemmatized_words = []\n","    for i in range(len(X_data_full)):\n","        test_data = lemmatize_words(X_data_full[i])\n","        X_data_full_lemmatized_words.append(test_data)\n","        \n","    return X_data_full_lemmatized_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLmefNAmakAW","colab_type":"code","colab":{}},"source":["fake_words.iloc[50,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZyJag3OsNpGv","colab_type":"code","colab":{}},"source":["#h=[\"break michael flynn crack testifi mueller trump himselfmichael flynn trump embattl former nation secur advi reportedli cave testifi robert mueller team trump collu russia accord abc news special report flynn plead guilti charg includ make fal statement fbi importantli admit plea offici trump transit team direct contact russian officialsfurthermor accord cnn david wright twitter report brian ross report abc news said flynn also say prepar testifi trump order direct make contact russian contradict donald trump said point brianross report michael flynn prepar testifi presid trump candid donald trump order direct make contact russian contradict donald trump said point david wright davidwrightcnn decemb brianross well told flynn made deci cooper last hour distraught deci feel right thing countri david wright davidwrightcnn decemb brianross face huge legal bill million dollar said final go reason expect put hou market face seriou financ problem david wright davidwrightcnn decemb part flynn issu statement say follow action acknowledg court today wrong faith god work set thing right guilti plea agreement cooper special counsel offic reflect deci made best interest famili countri accept full respon action white hou said mere got flynn fire first place zero effect trump har de har har make us laugh hard hurt merri christma donald trump entir treason famili administr hope like orang jumpsuit featur imag via chip somodevillagetti imag\"]\n","\n","f=[fake_words.iloc[50,0]]\n","t=[true_text.iloc[50,0]]\n","\n","h_data=final(t)\n","\n","rfc.predict(h_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_qm6dYbQ7tv","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}